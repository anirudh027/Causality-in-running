{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the columns that are not needed\n",
    "cols_to_keep = ['activity_id', 'timestamp', 'total_power', 'air_power', 'form_power', 'heart_rate', 'cadence', 'stride_length', 'speed', 'vertical_oscillation','ground_time','peak_vertical_grf','leg_spring','grade','temperature','humidity','distance','duration','apparel_model_brand','apparel_model_name','apparel_size_primary','apparel_size_region','env_temperature','env_humidity','weight_kg','elevation','flight_time_ms','stride_time_ms','duty_factor_perc','vertical_stiffness_kN_m','altitude_change_m','stryd_air_power_w','df_activities_user_id','ts','critical_power','stryds','env_dew_point','env_wind_bearing','env_wind_speed','env_wind_gust','number_steps','bsa_m2','fsa_m2','mmp_10','mmp_30','mmp_60','mmp_120','mmp_300','mmp_600','mmp_1800','mmp_3600','mmp_5400','mmp_7200','mmp_10_w_kg','mmp_30_w_kg','mmp_60_w_kg','mmp_120_w_kg','mmp_300_w_kg','mmp_600_w_kg','mmp_1800_w_kg','mmp_3600_w_kg','mmp_5400_w_kg','mmp_7200_w_kg','age','gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_final_data = pd.read_csv('full_final_data.csv')\n",
    "full_final_data = pd.read_parquet('full_final_data.parquet', engine='fastparquet')\n",
    "\n",
    "filtered_df = full_final_data.copy()\n",
    "filtered_df = filtered_df[cols_to_keep].dropna()\n",
    "filtered_df = filtered_df[filtered_df['gender']!='other']\n",
    "filtered_df['apparel_model'] = filtered_df[\"apparel_model_brand\"] + ' ' + filtered_df[\"apparel_model_name\"]\n",
    "filtered_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Additional Filters, if necessary\n",
    "\n",
    "# #filtering the data\n",
    "# filtered_df = filtered_df.groupby('activity_id').filter(lambda x: (x['speed'].between(1, 10)).all())\n",
    "# filtered_df = filtered_df.groupby('activity_id').filter(lambda x: (x['vertical_oscillation'].between(1, 25)).all())\n",
    "# filtered_df = filtered_df.groupby('activity_id').filter(lambda x: (x['ground_time'].between(100, 420)).all())\n",
    "\n",
    "# filtered_df = filtered_df.groupby('activity_id').filter(lambda x: (x['grade'].between(-0.20, 0.20)).all())\n",
    "# filtered_df = filtered_df.groupby('activity_id').filter(lambda x: (x['leg_spring'].between(1, 20)).all())\n",
    "# filtered_df = filtered_df.groupby('activity_id').filter(lambda x: (x['cadence'].between(120, 210)).all())\n",
    "# filtered_df = filtered_df.groupby('activity_id').filter(lambda x: (x['stride_length'].between(0.3, 2.5)).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the shoe type to the data\n",
    "    # Step 1: Define your lists of super shoes and non-super shoes\n",
    "super_shoes = [\n",
    "    'Adidas Adizero Adios Pro', 'Adidas Adizero Pro', 'Adidas Adizero Prime X',\n",
    "    'Adidas Adizero Prime', 'Adidas Adizero Adios Pro 2', 'Altra Vanish Tempo',\n",
    "    'Altra Vanish Carbon', 'ASICS Metaspeed Sky', 'ASICS Magic Speed', 'ASICS Metaspeed Edge',\n",
    "    'ASICS MetaRacer', 'Brooks Hyperion Elite', 'HOKA ONE ONE Carbon X', 'HOKA ONE ONE Rocket X',\n",
    "    'New Balance FuelCell SuperComp Trainer', 'New Balance FuelCell SuperComp Elite',\n",
    "    'New Balance FuelCell RC Elite', 'Nike Vaporfly Next %', 'Nike Alphafly Next %',\n",
    "    'Nike Air Zoom Tempo Next %', 'Nike Vaporfly 4%', 'On Cloudboom', 'On Cloudboom Echo',\n",
    "    'Saucony Endorphin Pro'\n",
    "]\n",
    "\n",
    "non_super_shoes = [\n",
    "    'Adidas Ultraboost', 'Adidas Solar Glide', 'Adidas Solar Boost', 'Adidas SL20',\n",
    "    'Adidas Supernova', 'Adidas Ultraboost 21', 'Altra Lone Peak', 'Altra Escalante', \n",
    "    'Altra Torin', 'ASICS Novablast', 'ASICS GT 2000', 'ASICS Gel Nimbus', 'ASICS Gel Cumulus', \n",
    "    'ASICS DynaFlyte', 'ASICS Gel Kayano', 'ASICS Gel DS Trainer', 'ASICS Gel Noosa Tri',\n",
    "    'ASICS GlideRide', 'ASICS Superblast', 'ASICS EvoRide', 'ASICS Noosa Tri', 'ASICS Noosa',\n",
    "    'Brooks Ghost', 'Brooks Launch', 'Brooks Adrenaline', 'Brooks Hyperion Tempo', 'Brooks Levitate', \n",
    "    'Brooks Glycerin', 'Brooks Ricochet', 'Brooks Defyance', 'HOKA ONE ONE Clifton', \n",
    "    'HOKA ONE ONE Mach', 'HOKA ONE ONE Speedgoat', 'HOKA ONE ONE Rincon', 'HOKA ONE ONE Arahi',\n",
    "    'HOKA ONE ONE Challenger', 'HOKA ONE ONE Bondi', 'HOKA ONE ONE Tecton X',\n",
    "    'New Balance Fresh Foam 1080', 'New Balance 880', 'New Balance FuelCell Rebel', \n",
    "    'New Balance 1500', 'New Balance 890', 'Nike Pegasus', 'Nike Pegasus Turbo', 'Nike Infinity Run', \n",
    "    'Nike Invincible', 'Nike Vomero', 'Nike Terra Kiger', 'Nike Pegasus Shield', 'On Cloudflow', \n",
    "    'On Cloudflyer', 'On Cloudmonster', 'On Cloudstratus', 'On Cloudsurfer', 'Saucony Endorphin Speed', \n",
    "    'Saucony Kinvara', 'Saucony Ride', 'Saucony Triumph', 'Saucony Peregrine', 'Saucony Endorphin Shift', \n",
    "    'Saucony Guide', 'Saucony Fastwitch', 'Saucony Munich'\n",
    "]\n",
    "\n",
    "\n",
    "filtered_df = filtered_df[filtered_df['apparel_model'].isin(super_shoes+non_super_shoes)].reset_index(drop=True)\n",
    "\n",
    "    # Step 2: Define a function to categorize each shoe\n",
    "def categorize_shoe(model):\n",
    "    if model in super_shoes:\n",
    "        return 'Super Shoe'\n",
    "    elif model in non_super_shoes:\n",
    "        return 'Non-Super Shoe'\n",
    "    else:\n",
    "        return 'Unknown'  # In case the model is not in either list\n",
    "\n",
    "    # Step 3: Apply the function to your DataFrame to create a new column\n",
    "filtered_df['shoe_type'] = filtered_df['apparel_model'].apply(categorize_shoe)\n",
    "\n",
    "filtered_df = filtered_df[filtered_df[\"apparel_size_primary\"]!= 255]\n",
    "def convert_size(size, region, target_region=\"us\"):\n",
    "    if region == \"uk\":\n",
    "        if target_region == \"us\":\n",
    "            return size + 2\n",
    "        elif target_region == \"eu\":\n",
    "            return size + 33\n",
    "    elif region == \"us\":\n",
    "        if target_region == \"uk\":\n",
    "            return size - 2\n",
    "        elif target_region == \"eu\":\n",
    "            return size + 31\n",
    "    elif region == \"eu\":\n",
    "        if target_region == \"us\":\n",
    "            return size - 31\n",
    "        elif target_region == \"uk\":\n",
    "            return size - 33\n",
    "    return size  # Return the size unchanged if no conversion is needed\n",
    "\n",
    "# Apply conversion\n",
    "filtered_df['normalised_shoe_size'] = filtered_df.apply(lambda row: convert_size(row['apparel_size_primary'], row['apparel_size_region'], target_region=\"us\"), axis=1)\n",
    "\n",
    "# Adding the intensity to the data\n",
    "filtered_df[\"relative_total_power\"] = filtered_df[\"total_power\"] / filtered_df[\"weight_kg\"]\n",
    "filtered_df[\"max_mmp600\"] = filtered_df.groupby('df_activities_user_id')['mmp_600_w_kg'].transform('max')\n",
    "filtered_df[\"intensity\"] = filtered_df[\"relative_total_power\"]/filtered_df[\"max_mmp600\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding quarters to the data\n",
    "def assign_quartiles(group):\n",
    "    group = group.sort_values(by='timestamp')  # Sort each group by timestamp\n",
    "    try:\n",
    "        group['quartile'] = pd.qcut(group['timestamp'], q=4, labels=[1, 2, 3, 4], duplicates='drop')\n",
    "    except ValueError as e:\n",
    "        # Handle cases where fewer than 4 bins are created due to duplicates\n",
    "        group['quartile'] = pd.qcut(group['timestamp'], q=len(group['timestamp'].unique()), labels=False) + 1\n",
    "    group['heart_rate_diff'] = group['heart_rate'].diff()\n",
    "    return group\n",
    "\n",
    "filtered_df = filtered_df.groupby('activity_id').apply(assign_quartiles)\n",
    "\n",
    "# Define a custom aggregation function\n",
    "def custom_agg(x):\n",
    "    if pd.api.types.is_numeric_dtype(x):\n",
    "        return np.mean(x)\n",
    "    else:\n",
    "        return x.mode()[0] if not x.mode().empty else np.nan\n",
    "    \n",
    "all_data_full = filtered_df.reset_index(drop=True).groupby([\"activity_id\",'quartile']).agg(custom_agg).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "# all_data_full.to_csv('all_data_full.csv', index=False)\n",
    "\n",
    "# Save the data according to brand\n",
    "# adidas_data_full = all_data_full[all_data_full['apparel_model_brand']=='Adidas']\n",
    "# adidas_data_full.to_csv('adidas_data_full.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
